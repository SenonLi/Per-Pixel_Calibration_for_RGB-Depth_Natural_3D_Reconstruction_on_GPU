% Chapter 4
\chapter{Results of Calibration and \gls{3D} Reconstruction} % Main chapter title
\label{chapterCaliResultsReconstruction} % For referencing the chapter elsewhere, use \ref{sens_CalibrationSystem} 
As discussed in section~\ref{perpixelDataCollection}, the per-pixel calibration method is supposed to handle both of radial dominated lens distortions and depth distortion. A two-dimensional high-order polynomial model would be employed to remove lens distortions and generate estimated (\(\gls{worldX},\, \gls{worldY}\))s from image space (\(R, \, C\))s. And the depth distortion would be removed during the per-pixel \(D\) to \(\gls{worldZ}\) mapping during 3D reconstruction. In this chapter, we will show how well the lens distortions and depth distortion are removed by compare both of intuitive image data and quantitative numerical data, before and after calibration, in both of Matlab prototypes and real-time 3D reconstructions. 
%%
\section{Calibration and Analysis}
\label{sectionPrototypeTwoDtransformation} 
%
\indent
Figure~\ref{MatlabPrototpyeOfHighOrder} shows the Matlab prototypes of the simulated original image, world space \(X^WY^W\) plane result after a two-dimensional \(1^{st}\) order polynomial transformation, \(2^{nd}\) order polynomial and \(4^{th}\) order polynomial transformation. With squared-shaped distributed points (\(C,\, R\))s extracted from image streams, fig.~\subref*{Original_MatlabPrototype} recovers the original distorted image in Matlab. Using a mathematical distortion (\(d\)) measurement \cite{distortionMeasurement_2012}
%
\begin{equation}
d (\%)=  e*100/L ,
\label{mathematicalDistortion}
\end{equation}%
%
\noindent
we can get the original distortion \(d_0 = (R3 - R1) / (C2 -C1) = (403 - 393) / (492 - 20) = 2.1\%\). Figure~\ref{First_MatlabPrototype} shows estimated world space \(X^WY^W\) plane after a two-dimensional \(1^{st}\) order polynomial transformation, whose distortion \(d_1 = (Y1 - Y3) / (X2 -X1) = [-3.772 - (-4.004)] / [5.713 - (-4.735)] = 2.2\%\). As we may have expected, the distortion \(d_1\) is not getting smaller at all. %
\\\indent
Figure~\ref{Second_MatlabPrototype} and Fig.~\ref{Fourth_MatlabPrototype} show the transformed world space \(X^WY^W\) plane images after the \(2^{nd}\) order and \(4^{th}\) order polynomial transformation respectively, from which we can get \(d_2 = [-3.807 - (-4.035)] / [5.779 - (-4.8)] = 2.1\%\) and \(d_4 = [-3.936 - (-3.992)] / [5.923 - (-4.928)] = 0.516\%\). %
%
\begin{figure}[!t]
\hspace*{-0.3cm}
\centering
\subfloat[Image Space][Image Space]{
\includegraphics[width=0.5\textwidth]{Original_MatlabPrototype}
\label{Original_MatlabPrototype}}
%\qquad
\subfloat[\(1^{st}\) Order][\(1^{st}\) Order]{
\includegraphics[width = 0.5\textwidth]{First_MatlabPrototype}
\label{First_MatlabPrototype}}

\subfloat[\(2^{nd}\) Order][\(2^{nd}\) Order]{
\includegraphics[width=0.5\textwidth]{Second_MatlabPrototype}
\label{Second_MatlabPrototype}}
%\qquad
\subfloat[\(4^{th}\) Order][\(4^{th}\) Order]{
\includegraphics[width = 0.5\textwidth]{Fourth_MatlabPrototype}
\label{Fourth_MatlabPrototype}}
%
\caption{\(\gls{worldX}\)\(\gls{worldY}\) Matlab Polynomial Prototype}
\label{MatlabPrototpyeOfHighOrder}
\end{figure}%
%
%
It is straightforward to tell that, \(d_4\) is much smaller than \(d_0\) and Fig.~\ref{Fourth_MatlabPrototype} intuitively shows a satisfying undistorted image. From eqn.~(\ref{secondOrderPolynomial}) and eqn.~(\ref{fourthOrderPolynomial}), we know that the second order polynomial mapping has $2\times6=12$ parameters, and the fourth order polynomial mapping has $2\times15=30$ parameters. The higher order polynomial we use, the better radial distortion we are able to correct. In the meantime, the distortion removal model will have more parameters to calculate, and need more calibrating points to train the model.%
%
%%
\\\indent
\begin{figure}[!t]
\centering
\hspace*{-0.3cm}
\subfloat[Before transformation][Before transformation]{
\includegraphics[width=0.5\textwidth, height = 0.425\textwidth]{BeforeRectification_Single_NIR}
\label{BeforeRectification_Single_NIR}}
%
\subfloat[Perspective Correction][Perspective (\(1^{st}\))]{
\includegraphics[width = 0.5\textwidth, height = 0.425\textwidth]{Perspective_QtScreenShot}
\label{Perspective_QtScreenShot}}
%
\\%\qquad
\hspace*{-0.3cm}
\subfloat[\(2^{nd}\) Order][\(2^{nd}\) Order]{
\includegraphics[width = 0.5\textwidth, height = 0.425\textwidth]{Second_QtScreenShot}
\label{Second_QtScreenShot}}
%
\subfloat[\(4^{th}\) Order][\(4^{th}\) Order]{
\includegraphics[width = 0.5\textwidth, height = 0.425\textwidth]{Fourth_QtScreenShot}
\label{Fourth_QtScreenShot}}
%
\caption{\gls{NIR} Stream High Order Polynomial Transformation}
\label{HighOrderNearIRRectification}
\end{figure}%
%
By applying those two-dimensional polynomial models into real-time streams transformation, we can get the transformed stream images. As shown in Fig.~\ref{HighOrderNearIRRectification}, the outlines of the transformed steam images are same with Matlab prototypes in Fig.~\ref{MatlabPrototpyeOfHighOrder}. It is easy to tell that the \(4^{th}\) order polynomial surface mapping is much better than the second order, and a higher order than \(4^{th}\) should be more accurate. However, as the order of the polynomial mapping goes higher, the number of parameters also get larger and larger, which costs more calculations and requires more data (coordinate-pairs) for training the transformation model. Considering that a \(5^{th}\) order polynomial mapping will have much more  parameters ($2\times21=42$) to calculate while may not enhance much accuracy, we choose the \(4^{th}\) order polynomial as the main mapping model to get \(X^WY^W\) values from \(RC\). Limited by the static dot pattern, fewer and fewer dot-clusters could be observed by the camera as the camera getting closer to the dot pattern. Practically, \(4^{th}\) order calibration is replaced by \(2^{nd}\) order to guarantee a robust software when the observed dot-clusters are too few to train the transformation model.
\\\indent
%%
%%
%% %% mapping model parameters determination
%
The two-dimensional high-order polynomial mapping model will be applied in the first calibration step of \(\gls{worldX} \gls{worldY} \gls{worldZ}+\gls{D}\) frames data collection. Figure~\ref{Data63FranesForLUT} shows 63 frames of collected \(\gls{worldX} \gls{worldY} \gls{worldZ}\), which gives an pyramid shape of a camera sensor's undistorted world space field of view. For each single pixel, its field of view is a beam, which could be mathematically expressed as equation~(\ref{kaiBeamEquationCh3}). Some sample beams are shown in Fig.~\ref{SampleBeams_NearIR}, whose beam equation parameters \(c\)/\(d\)/\(e\)/\(f\) are determined as the best-fit totally by the collected undistorted data. 
\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth, height= 0.55\textwidth]{Data63FranesForLUT}
\caption{63 Frames \gls{NIR} Calibrated \gls{3D} Reconstruction}
\label{Data63FranesForLUT}
\end{figure}%
%
%
\begin{figure}[t]
\centering
\includegraphics[width=0.6\textwidth]{SampleBeams_NearIR}
\caption{Sample Beams of Calibrated \gls{NIR} Field of View}
\label{SampleBeams_NearIR}
\end{figure}
%
%As discussed in section~\ref{sectionDataProcessLUTgeneration}, the per-pixel \(\gls{D}\) to \(\gls{worldZ}\) mapping is linear. 
As for the handling of depth distortion, a pre-process of depth stream is needed. Considering that there will be irregular noises (different with the regular depth distortion) in the depth stream, especially those brought in by the defects affected by huge transitions of color (or intensity), we will find a best-fit plane for each frame based on its \(\gls{worldX} \gls{worldY} \gls{D}\) data and throw away 10\% worst pixels that are far away from the best-fit plane. Figure~\ref{depthDistortionComparisonBeforeAfterCalibration} shows 10 frames of flat-shading 3D reconstructions in Matlab for both of before and after calibration. In Fig.~\subref*{cameraFramesInWorldSpace}, the 3D reconstructions are based on camera space coordinates \(\gls{cameraX} \gls{cameraY} \gls{cameraZ}\) generated by eqn.~(\ref{proportionalBeamEqn}) and (\ref{parametersABofProportional}); which are then transformed into world space for a better comparison with the calibrated reconstruction, using best fit rotation and translation matrix based on corresponding \(\gls{worldX} \gls{worldY} \gls{worldZ}\). In Fig.~\subref*{worldFramesInWorldSpace}, the 3D reconstructions are based on world space coordinates generated by eqn.~(\ref{kaiBeamEquationCh3}) and (\ref{fromD_To_Z}). We can tell that the 10 frames in the LUT calibrated 3D reconstructions are a little bit thinner than that of raw Pin-Hole reconstructions, which means the per-pixel calibration method has positive effect on the removal of depth distortion. The depth distortion removal in real-time is shown in section~\ref{sectionRealTimeReconstruction} Fig.~\ref{depthDistortionCalibrationBeforeAfter}.

\begin{figure}[t]
\centering
\hspace*{-0.3cm}
\subfloat[Raw Pin-Hole Reconstructions in World Space         through best-fit rotation and translation]{
\includegraphics[width=0.5\textwidth]{cameraFramesInWorldSpace}
\label{cameraFramesInWorldSpace}}
%
\subfloat[Calibrated \gls{LUT} based Reconstructions]{
\includegraphics[width = 0.5\textwidth]{worldFramesInWorldSpace}
\label{worldFramesInWorldSpace}}
%
\caption{Depth Distortion, before and after Calibration}
\label{depthDistortionComparisonBeforeAfterCalibration}
\end{figure}%
%
%\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Real-Time \gls{3D} Reconstruction on \gls{GPU}}
\label{sectionRealTimeReconstruction}
%
The \gls{3D} Reconstruction of undistorted \(\gls{worldX}/\gls{worldY}/\gls{worldZ}\) in real-time is the final aim of a \gls{3D} camera's calibration. In the traditional camera calibration method, which consist of one pinhole-camera matrix to generate raw world space \gls{3D} coordinates and another model for lens distortion removal, three big transformations are needed to generate the world space coordinates: from 2D distorted image space to 2D undistorted image space, then to \gls{3D} camera space, and finally to \gls{3D} world space. For every single pixel's processing, it needs 5 parameters from distortion removal model for the first step non-linear calculation, and then a $3\times3$ intrinsic matrix to get its camera space coordinates, and a $3\times4$ extrinsic matrix to finally acquire the world space coordinates. The \gls{3D} reconstruction after the traditional calibration requires a lot of calculations for every single pixel, and \emph{depth distortion} is not corrected at all.
\\\indent
%
Using the proposed per-pixel calibration method, only three linear calculations with six parameters are needed to determine the world space coordinates for every single pixel. Two parameters \(e/f\) are utilized to generate world space \(\gls{worldZ}\), as expressed in eqn.~(\ref{fromD_To_Z}). And the other four parameters \(a/b/c/d\) are applied to get \(\gls{worldX}/\gls{worldY}\) respectively based on eqn.~(\ref{kaiBeamEquationCh3}). In this way, there is no need to calculate any non-linear equation for distortions removal, and the camera space is totally left aside. Combining two equations together, the undistorted \gls{3D} world coordinates (\(\gls{worldX}, \, \gls{worldY}, \, \gls{worldZ}\)) for every single pixel could be looked up based on \(\gls{D}\) from a \(\gls{imageColumn}\)-by-\(\gls{imageRow}\)-by-\(6\) look-up table. Figure~\ref{perPixelCalibrationBeforeAfter} shows how lens distortions are moved, and Fig.~\ref{depthDistortionCalibrationBeforeAfter} shows how the \emph{depth distortion} is removed by per-pixel \(\gls{D}\) to \(\gls{worldZ}\) mapping.
\begin{figure}[t]
\centering
\hspace*{-0.3cm}
\subfloat[Raw (Distorted)]{
\includegraphics[width=0.5\textwidth, height = 0.425\textwidth]{distortedRGB}
\label{distortedRGB}}
%
\subfloat[Calibrated]{
\includegraphics[width = 0.5\textwidth, height = 0.425\textwidth]{CalibratedRGB}
\label{CalibratedRGB}}
%
\caption{Lens-Distortions Removal by Per-Pixel Calibration Method}
\label{perPixelCalibrationBeforeAfter}
\end{figure}%
%
\begin{figure}[t]
\centering
\hspace*{-0.3cm}
\subfloat[Raw]{
\includegraphics[width=0.3\textwidth, height = 0.425\textwidth]{distortedSideViewRGB}
\label{distortedSideViewRGB}}
\qquad%
\subfloat[Calibrated]{
\includegraphics[width = 0.3\textwidth, height = 0.425\textwidth]{CalibratedSideView}
\label{CalibratedSideView}}
%
\caption{Depth-Distortions Removal by Per-Pixel Calibration Method}
\label{depthDistortionCalibrationBeforeAfter}
\end{figure}%
%
%%
%
%

















