% Chapter 3
\chapter{Conclusion and Future Work} % Main chapter title
\label{chapterConclusionAndFutureWork} 

\section{Conclusion}
\indent
The depth sensor technologies opens a new epoch for 3D markets. Ever since Microsoft brought the low-cost depth camera Kinect into consumer market, RGB-D cameras are famous for their 3D reconstruction applications in research areas of HCI. Without calibration, the horizontal and vertical field of view (FoV) could help generate 3D reconstruction in camera space naturally on GPU through proportional calculations in fragment shader, which however is badly deformed by the lens distortions and \emph{depth distortion}. Traditionally, the camera calibration is done based on a pinhole-camera model and a high-order distortion removal model. But there will be a lot of calculations in the fragment shader to process parameters from both of pinhole-camera matrix and distortions' removal vector. In order to get rid of both the lens distortion and the \emph{depth distortion} while still able to do simple calculations in the GPU fragment shader, a new solution is needed.
\\\indent
In this thesis, a novel per-pixel calibration method with look-up table based 3D reconstruction in real-time is introduced, using a rail calibration system. This rail calibration system offers possibilities of collecting infinite calibration points with dense distributions that can cover all pixels in a sensor. Not only lens distortions, but \emph{depth distortion} can also be handled by a per-pixel \(D\) to \(Z^W\) mapping with data along \(Z^W\)-axis collected on the rail. Instead of utilizing the traditional pinhole camera model, two polynomial mapping models are employed in this calibration method. The first model is the two-dimensional \(4^{th}\) order polynomial mapping from \(R/C\) to \(X^W/Y^W\) during the frames data collection, which takes care of the removal of lens distortions; and the second model is the linear mapping from \(D\) to \(Z^W\), which can handle \enquote{depth distortion}. The method consists of two big steps: \(X^WY^WZ^W+D\) data collection and mapping parameters determination. \(D\) is simply from depth streams. \(Z^W\) is from external based on the camera's position on the rail. And the undistorted (\(X^W, \, Y^W\)) are from the transformation of \(R/C\) by a \(4^{th}\) order polynomial mapping model, during which lens-distortions could be removed. This method is claimed as \enquote{data-based} calibration method, because both of the two mapping models are determined and calculated by real streams data from the camera. 
\\\indent
With the fewest calculations, the undistorted 3D world coordinates (\(X^\text{W}, \, Y^\text{W}, \, Z^\text{W}\)) for every single pixel could be looked up in real-time based on \(D\) from a \(column\)-by-\(row\)-by-\(6\) look-up table. Only three linear equations with six parameters need to be calculated in the fragment shader. Two out of six parameters \(a/b\) are to determine the per-pixel \(Z^W\), which is generated from per-pixel depth value \(D\); and the other four parameters \(c/d/d/f\) are to determine the per-pixel \(X^W/Y^W\) respectively, which are mapped by per-pixel linear beam equation from the per-pixel \(Z^W\). Note that \enquote{real-time} here means being able to show an undistorted frame in 3D world space before the start of the second frame processing, and this LUT based 3D reconstruction method can realize real-time very well. The data-based per-pixel calibration method could be applied universally on any RGB-D cameras. With the alignment of RGB pixels using a pinhole camera matrix, it could even work on a combination of a random Depth sensor and a random RGB sensor.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%                                                                %%%%%%%%%%%%%%%%%
%%%%%%%%%%     5.2  Future Work                                    %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%                                                                %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}
%
The rail calibration system and per-pixel calibration method with LUT-based real-time 3D reconstruction could be applied universally to all kinds of RGB-D cameras. With a more precise calibration system and corresponding DIP technologies, there could be a huge improvement space for calibration accuracy. Hardware improvement is sometimes more important than a software updating. Concretely the system in the lab now can only handle a working distance \(Z^W\) from 1.165m to 2.565m. Considering that the depth resolution deteriorates notably with depth, it might not be a simple linear relationship from \(D\) and \(Z^W\), when the depth \(D\) value goes much further than the limit of the rail. In that case, the per-pixel \(D\) to \(Z^W\) mapping could be changed from singular linear to segmented mapping function when \(D\) gets larger than a certain value.
\\\indent
Not only the rail, the planar pattern could also be changed based on the resolution of the camera (\textit{e.g.}, size of the dots and side distance of the unit square-shaped of the uniform grid could be larger when the camera has a supper high resolution). A two dimensional object is totally enough for calibrating KinectV2, because the NearIR stream have same pixels' distortions with the depth stream. However, if NearIR stream could not be used to extract points \(R/C\) in image space (\textit{e.g.} a structured light based depth sensor) while the size (pixel numbers) of the RGB sensor does not share the same one with the depth sensor, we could make the planar pattern into a \enquote{three dimension mode}: change the printed dots into real wholes such that the depth stream could detect a difference between the \enquote{dots} and \enquote{white background}. 
\\\indent
Instead of using a laser distance measurer to manually input \(Z^W\) into every frame at the very beginning of the frame data collection, we could add a tracking module onto the rail to active the frame collection by software. In this way, not only \(Z^W\) could be traced by the tracking module, but also the frame collection could be automatically done by the activation from the tracking module, recording one frame data after every certain distance of movement has been detected. 
\\\indent
Besides the hardware enhancement, softwares like DIP process can also be improved. The order of polynomial model that help map from image space \(R/C\) to world space \(X^W/Y^W\) could also be heightened for a better accuracy, as long as there will be enough calibrating points to offer the coordinates pairs of both image space and world space. Considering the possible lens-distortions of the RGB sensor, the high order polynomial mapping could also be applied into the color values' alignment from RGB sensor pixels to depth sensor pixels.
































