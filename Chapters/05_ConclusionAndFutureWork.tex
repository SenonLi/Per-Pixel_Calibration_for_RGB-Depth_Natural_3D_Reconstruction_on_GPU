% Chapter 3
\chapter{Conclusion and Future Work} % Main chapter title
\label{chapterConclusionAndFutureWork} 
\indent
The depth sensor technologies opens a new epoch for \gls{3D} markets. Ever since Microsoft brought the low-cost depth camera Kinect into consumer market, \gls{RGBD} cameras are famous for their \gls{3D} reconstruction applications in research areas of \gls{HCI}. Graphic processing units (GPUs) can solve large data parallel problems at a higher speed than the traditional CPU, while being more affordable and energy efficient \cite{GPUadvantage_2015}. With floating point calculation and matrix operation, GPUs are specialized many-core processors that are optimized for graphic processing \cite{GPUadvantage02_2013}. Without calibration, the horizontal and vertical \gls{FoV} could help generate \gls{3D} reconstruction in camera space naturally on \gls{GPU} through proportional calculations in fragment shader, which nevertheless is badly deformed by the lens distortions and \emph{depth distortion}. Due to image formation, some methods must be designed to establish the correct correspondence between raw images streams from the camera and the real-time \gls{3D} reconstruction feedback. Traditionally, the camera calibration is done based on a pinhole-camera model and a high-order distortion removal model. However, there will be a lot of calculations in the fragment shader to process parameters. There will be at least five intrinsic parameters from intrinsic camera matrix and another five parameters from distortion removal vector, while the distortion correction in this method requires multiplication calculations to achieve the high order polynomial. In order to get rid of both the lens distortion and the \emph{depth distortion} while still be able to do simple calculations on the \gls{GPU} fragment shader, a new solution is needed. %
%
%\section{Conclusion}
\\\indent
In this thesis, a novel per-pixel calibration method with look-up table based \gls{3D} reconstruction on \gls{GPU} in real-time is introduced, using a rail calibration system. This rail calibration system offers possibilities of collecting infinite calibration points with dense distributions that can cover all pixels in a sensor. Not only lens distortions, but \emph{depth distortion} can also be handled by a per-pixel \(\gls{D}\) to \(\gls{worldZ}\) mapping with data along \(\gls{worldZ}\)-axis collected on the rail. Instead of utilizing the traditional pinhole camera model, two polynomial mapping models are employed in this calibration method. The first model is the two-dimensional \(4^{th}\) order polynomial mapping from \(R/C\) to \(\gls{worldX}/\gls{worldY}\) during the frames data collection, which takes care of the removal of lens distortions; and the second model is the linear mapping from \(\gls{D}\) to \(\gls{worldZ}\), which can handle \enquote{depth distortion}. The method consists of two big steps: \(X^WY^WZ^W+\gls{D}\) data collection and mapping parameters determination. \(\gls{D}\) is simply from depth streams. \(\gls{worldZ}\) is from external based on the camera's position on the rail. And the undistorted (\(\gls{worldX}, \, \gls{worldY}\)) are from the transformation of \(R/C\) by a \(4^{th}\) order polynomial mapping model, during which lens-distortions could be removed. This method is claimed as \enquote{data-based} calibration method, because both of the two mapping models are determined and calculated by real streams data from the camera. 
\\\indent
With the fewest calculations, the undistorted \gls{3D} world coordinates (\(\gls{worldX}, \, \gls{worldY}, \, \gls{worldZ}\)) for every single pixel could be looked up in real-time based on \(\gls{D}\) from a \(\gls{imageColumn}\)-by-\(\gls{imageRow}\)-by-\(6\) look-up table. Only three linear equations with six parameters need to be calculated in the fragment shader. Two out of six parameters \(a/b\) are to determine the per-pixel \(\gls{worldZ}\), which is generated from per-pixel depth value \(\gls{D}\); and the other four parameters \(c/d/d/f\) are to determine the per-pixel \(\gls{worldX}/\gls{worldY}\) respectively, which are mapped by per-pixel linear beam equation from the per-pixel \(\gls{worldZ}\). Note that \enquote{real-time} here means being able to show an undistorted frame in \gls{3D} world space before the start of the second frame processing, and this \gls{LUT} based \gls{3D} reconstruction method can realize real-time very well. The data-based per-pixel calibration method could be applied universally on any \gls{RGBD} cameras. With the alignment of RGB pixels using a pinhole camera matrix, it could even work on a combination of a random Depth sensor and a random RGB sensor.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%                                                                %%%%%%%%%%%%%%%%%
%%%%%%%%%%     5.2  Future Work                                    %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%                                                                %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Future Work}
%
\\\indent
The rail calibration system and per-pixel calibration method with \gls{LUT}-based real-time \gls{3D} reconstruction could be applied universally to all kinds of \gls{RGBD} cameras. With a more precise calibration system and corresponding \gls{DIP} technologies, there could be a huge improvement space for calibration accuracy. Hardware improvement is sometimes more important than a software updating. Concretely the system in the lab now can only handle a working distance \(\gls{worldZ}\) from 1.165m to 2.565m. Considering that the depth resolution deteriorates notably with depth, it might not be a simple linear relationship from \(\gls{D}\) and \(\gls{worldZ}\), when the depth \(\gls{D}\) value goes much further than the limit of the rail. In that case, the per-pixel \(\gls{D}\) to \(\gls{worldZ}\) mapping could be changed from singular linear to segmented mapping function when \(\gls{D}\) gets larger than a certain value.
\\\indent
Not only the rail, the planar pattern could also be changed based on the resolution of the camera (\textit{e.g.}, size of the dots and side distance of the unit square-shaped of the uniform grid could be larger when the camera has a supper high resolution). A two dimensional object is totally enough for calibrating \gls{KinectV2}, because the \gls{NIR} stream have same pixels' distortions with the depth stream. However, if \gls{NIR} stream could not be used to extract points \(R/C\) in image space (\textit{e.g.} a structured light based depth sensor) while the size (pixel numbers) of the RGB sensor does not share the same one with the depth sensor, we could make the planar pattern into a \enquote{three dimension mode}: change the printed dots into real wholes such that the depth stream could detect a difference between the \enquote{dots} and \enquote{white background}. 
\\\indent
Instead of using a laser distance measurer to manually input \(\gls{worldZ}\) into every frame at the very beginning of the frame data collection, we could add a tracking module onto the rail to active the frame collection by software. In this way, not only \(\gls{worldZ}\) could be traced by the tracking module, but also the frame collection could be automatically done by the activation from the tracking module, recording one frame data after every certain distance of movement has been detected. 
\\\indent
Besides the hardware enhancement, softwares like \gls{DIP} process can also be improved. The order of polynomial model that help map from image space \(R/C\) to world space \(\gls{worldX}/\gls{worldY}\) could also be heightened for a better accuracy, as long as there will be enough calibrating points to offer the coordinates pairs of both image space and world space. Considering the possible lens-distortions of the RGB sensor, the high order polynomial mapping could also be applied into the color values' alignment from RGB sensor pixels to depth sensor pixels.
































