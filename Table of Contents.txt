

You focus on getting X and Y from Z, but you don’t mention the issues with getting D.  The point is that while these RGB+D systems work great for human-computer interaction, they aren’t the best at doing 3-D scanning of objects for applications in metrology.  Say something about the rise in 3D printing and the market for building 3D models from fused scans acquired by the sensor.  So for these 3D scanning objects, we want to use low-cost cameras, but their built in calibration just isn’t great.  At the same time, there are a lot of similarities in how these sensors work compared to structured light scanners.  So can we learn anything from the calibration procedure of Kai?  Is there anything we can do to Kai’s work to make that better and also apply it to off-the-shelf RGB+D sensors like the Prime Sense and the Kinect?


Lens distortion calibration does not cover corners, where offers the most distortion info

Chapter 1 Introduction
** Depth Camera Application (human-computer interaction)
** 3D reconstruction
** Structured Light Illumination
** Calibration

1.1 3D Reconstruction
1.2 Structured Light (SL) 3D Scanner Calibration
 (include 2D (pinhole perspective distortion + lens distortion) and 3D (abcd beams equation))
	Kai's real-time 3D reconstruction.
1.3 Contributions of this thesis
1.4 Summation

Chapter2: From Structured Light to 3D Scanner Calibration based on Pinhole Model
2.1 PinHole Camera Model (not able to handle non-linear distortion)
2.2 3D Reconstruction in Real-Time (Kai's calculation)
2.3 Shortcoming and Extensions (Systems Transformation)

Chapter 3 Data-Based Real-Time 3D Calibration 
3.1 Xw/Yw Rectifications
3.2 Zw Rectification
3.3 Data-Based XYZWRGB-D Look-Up Table
3.4 Real-Time analysis

Chapter 4 Calibration System for RGBD Cameras
4.1 Rail System
4.2 BLE Optical-Flow Tracking Module

Chapter 5 Conclusion and Future Work
5.1 Conclusion
5.2 Future Work
alignment from a random depth sensor to a random RGB sensor.







In the matlab script, what you call D is called P. You probably don’t need lines 32-36.  You also don’t need lines 40-42. Line 45 creates the LUT.  Lines 47-56 rebuilds Z from D so that you can see how good your LUT is by comparing Zp and Z.  Lines 58-72 saves the LUT to disk as a .tif file. If you resync to the github repository, you will see that I modified my software to use this LUT file format regardless of the sensor.  I still need to modify the Kinect shader though.














